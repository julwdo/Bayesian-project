\documentclass{Class/julia}

\usepackage{geometry}
\usepackage{graphicx} % To use \resizebox
\usepackage{array} % For custom column widths
\usepackage{calc} % To use \widthof

\usepackage{siunitx} % Formatting numbers in a table

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{multirow}
%\usepackage{placeins}
\usepackage{booktabs}
\usepackage{tablefootnote}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}

%\author{Julia Maria Wdowinska}
\date{} % Remove date from the title

\begin{document}

\begin{titlepage}
    \centering
    \vfill
    {\scshape\Large University of Milan \par}
    \vspace{0.5cm}
    {\scshape\large Faculty of Political, Economic and Social Sciences \par}
    \vspace{3cm}
    {\huge
    \textbf{A Bayesian Approach to Aggregate Insurance Claim Modeling} \\
    \vspace{0.5cm}
    \large Final Project in the Subject Bayesian Analysis \par}
    \vspace{2cm}
    {\large \textbf{Julia Maria Wdowinska} (43288A) \par}
    {\large \textbf{Edoardo Zanone} (33927A) \par}
    \vspace{0.5cm}
    {\large Data Science for Economics \par}
    {\large II Year \par}
    {\large Master’s Degree \par}
    \vfill
\begin{center}
\begin{figure}[h!]\centering
 \includegraphics[keepaspectratio=true,scale=0.2]{logo} \\
\end{figure}
\end{center}
\vfill
\begin{center}
{\small{We declare that this material, which we now submit for assessment, is entirely our own work and has not been taken from the work of others, save and to the extent that such work has been cited and acknowledged within the text of our work. We understand that plagiarism, collusion, and copying are grave and serious offences in the university and accept the penalties that would be imposed should I engage in plagiarism, collusion or copying. This assignment, or any part of it, has not been previously submitted by us or any other person for assessment on this or any other course of study.}}
\end{center}
\vfill
    {\large \today \par}
    \vfill
\end{titlepage}

\tableofcontents
%\newpage

\section{Introduction}

\section{Theoretical Background}

In insurance modeling, accurately estimating the aggregate value of claims is essential for premium calculation, risk assessment, and maintaining solvency of insurance providers. The aggregate claim value refers to the total amount paid out for claims over a specified period. This quantity is typically modeled using a compound distribution, which separately describes the frequency and severity of claims. The frequency component, representing the number of claims within the period, is often modeled using a Poisson distribution. The severity component, which captures the magnitude of individual claims, is commonly modeled using heavy-tailed distributions such as the Pareto or Gamma distributions.

The traditional framework for modeling such processes is grounded in frequentist statistics. In this approach, the parameters of the underlying distributions—such as the Poisson rate parameter \( \lambda \), or the shape and scale pa- rameters \( \alpha \) and \( \beta \) of the Pareto distribution—are considered fixed but unknown quantities. These parameters~are typically estimated from observed data using techniques such as maximum likelihood estimation (MLE).

In contrast, the Bayesian framework treats these parameters as random variables characterized by prior~distributions. This allows for the incorporation of prior knowledge or beliefs about the parameters before any~data is observed. The prior distribution is updated using Bayes' theorem upon observing data, resulting in a posterior distribution that reflects the updated beliefs. This posterior distribution forms the basis for inference and pre- diction.

Bayesian methods offer significant advantages in insurance modeling. They are well-suited for incorporating expert knowledge or external information, handling limited or noisy datasets, and accommodating complex hier- archical structures commonly found in insurance data. Moreover, Bayesian inference provides a coherent frame- work for uncertainty quantification by yielding full posterior distributions over model parameters.

Nevertheless, Bayesian methods also come with certain limitations. The choice of prior distribution can~introduce subjectivity, and computational complexity can be significant, especially in models with high dimensionality or non-conjugate priors. Despite these challenges, the flexibility and robustness of the Bayesian framework make it a valuable approach for modeling aggregate insurance claims.

\section{Replication of \citet{dudley2006bayesian}}

Building on the theoretical background, this project aims to replicate the Bayesian modeling approach proposed by \citet{dudley2006bayesian}. The analysis is based on a dataset of automobile insurance claims exceeding 1.5 million, col- lected over a five-year period. The data, originally reported by \citet{rytgaard1990pareto}, is presented in Table \ref{tab:1}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Insurance Claim Amounts Exceeding 1.5 Million (Data from \citealp{rytgaard1990pareto})}
\label{tab:1}
\begin{threeparttable}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{Year}}
*{5}{S[table-format=2.3]}
}
\hline
\textbf{Year} & \multicolumn{5}{c}{\textbf{Claim Amounts (in millions)}} \\ \hline
1 & 2.495 & 2.120 & 2.095 & 1.700 & 1.650 \\
2 & 1.985 & 1.810 & 1.625 & \textendash & \textendash \\
3 & 3.215 & 2.105 & 1.765 & 1.715 & \textendash \\
4 & \textendash & \textendash & \textendash & \textendash & \textendash \\
5 & 19.180 & 1.915 & 1.790 & 1.755 & \textendash \\ \hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item The threshold of 1.5 million corresponds to the retention level of an excess-of-loss insurance policy\tablefootnote{To manage risk exposure, insurers frequently employ reinsurance strategies, which help reduce their financial liability on large claims. Under such arrangements, if a claim amount \( y \) exceeds a predetermined threshold \( d \) (the retention), the insurer is responsible only for paying up to \( d \), while any excess \( y - d \) is covered by the reinsurer.}.
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Model Specification}

To model this dataset within a Bayesian framework, assumptions about the distributions of both the number of claims in year \( t \) (\( N_t \)) and the amount of the \( i \)-th claim in year \( t \) (\( Y_{i,t} \)) were necessary. Claims were assumed to occur randomly and independently at a constant rate over time, so \( N_t \) was modeled using a Poisson distribution. A Pareto distribution was selected for \( Y_{i,t} \), as a heavy-tailed loss distribution was needed to account for the fact that individual claim amounts are positive and may include large outliers. That is,
\begin{align*}
N_t &\sim \text{Poisson}(\theta), \quad 0 < \theta < \infty, \\
Y_{i,t} &\sim \text{Pareto}(\alpha, \beta), \quad \alpha > 0, \quad 0 < \beta < y.
\end{align*}

\noindent The \text{Pareto}\((\alpha, \beta)\) distribution with support \( [\beta, \infty) \) was particularly suitable in this context, as it was employed to model claim amounts exceeding a specified threshold.

In addition, the following assumptions were made:

\begin{itemize}
\item \( N_t \) are independently and identically distributed (i.i.d.) across \( t \),
\item \( Y_{i,t} \) are i.i.d.\ across both \( i \) and \( t \),
\item \( N_t \) and \( Y_{i,t} \) are independent for all \( i \) and \( t \).
\end{itemize}

\noindent Under these assumptions, the aggregate claim amount in year \( t \) was defined as
\[
S_t = Y_{1,t} + Y_{2,t} + \cdots + Y_{N_t,t}.
\]

Next, prior distributions for the parameters \( \alpha \), \( \beta \), and \( \theta \) were specified. Due to limited prior knowledge about their true values—beyond the assumption that they are strictly positive—vague Gamma priors\footnote{Each of these Gamma priors has a variance of \(10^8\), implying minimal prior influence so that most of the information about~the parameters is derived from the dataset. Additionally, the Gamma distribution is conjugate to both the Poisson and Pareto likeli- hoods, facilitating analytical tractability in Bayesian inference.} were chosen:
\[
\alpha \sim \text{Gamma}(1, 0.0001), \quad \beta \sim \text{Gamma}(1, 0.0001), \quad \theta \sim \text{Gamma}(1, 0.0001),
\]
with the constraint \( 0 < \beta < \min\{y_{i,t}\} \) to ensure validity of the Pareto distribution.

Finally, the posterior distributions were derived. First, the joint posterior distribution of \( (\alpha, \beta) \) was obtained via Bayes' theorem\footnote{Here, assuming that \( \alpha \) and \( \beta \) are independent, the joint prior \( \pi(\alpha, \beta) \) was computed as \( \pi(\alpha) \cdot \pi(\beta) \).}:
\begin{align*}
\pi(\alpha, \beta \mid \mathbf{y}) &\propto \pi(\alpha) \cdot \pi(\beta) \cdot f(\mathbf{y} \mid \alpha, \beta) \\
&\propto 0.0001 \cdot \exp(-0.0001 \alpha) \cdot 0.0001 \cdot \exp(-0.0001 \beta) \cdot \prod_{i=1}^{n} \frac{\alpha \beta^\alpha}{y_i^{\alpha+1}} \\
&\propto \exp(-0.0001 \alpha) \cdot \exp(-0.0001 \beta) \cdot \alpha^n \cdot \beta^{n \alpha} \left( \prod_{i=1}^{n} y_i \right)^{-(\alpha + 1)} \\
&\propto \alpha^n \cdot \exp(-0.0001 \alpha) \cdot \left( \prod_{i=1}^{n} y_i \right)^{-\alpha} \cdot \beta^{n \alpha} \cdot \exp(-0.0001 \beta) \\
&\propto \alpha^n \cdot \exp\left( - \left( 0.0001 + \sum_{i=1}^{n} \ln(y_i) \right) \alpha \right) \cdot \beta^{n \alpha} \cdot \exp(-0.0001 \beta)
\end{align*}

\noindent As a result, the full conditional posterior distributions of \( \alpha \) and \( \beta \) were as follows:
\begin{align*}
\pi(\alpha \mid \beta, \mathbf{y}) &\propto \alpha^n \cdot \exp\left( - \left( 0.0001 - n \ln(\beta) + \sum_{i=1}^{n} \ln(y_i) \right) \alpha \right), \\
\pi(\beta \mid \alpha, \mathbf{y}) &\propto \beta^{n \alpha} \cdot \exp(-0.0001 \beta),
\end{align*}

\noindent which implied that:
\begin{align*}
\alpha \mid \beta, \mathbf{y} &\sim \text{Gamma}\left(n+1, \sum_{i=1}^{n} \ln(y_i) - n \ln(\beta) + 0.0001\right), \\
\beta \mid \alpha, \mathbf{y} &\sim \text{Gamma}(n \alpha + 1, 0.0001).
\end{align*}

Similarly, the posterior distribution of \( \theta \) was obtained via Bayes' theorem:
\begin{align*}
\pi(\theta \mid \mathbf{n}) &\propto \pi(\theta) \cdot f(\mathbf{n} \mid \theta) \\
&\propto \exp(-0.0001 \theta) \cdot \prod_{t=1}^{T} \left( \theta^{n_t} \cdot \exp(-\theta) \right) \\
&\propto \exp(-0.0001 \theta) \cdot \theta^{\sum_{t=1}^{T} n_t} \cdot \exp(-5 \theta) \\
&\propto \exp(-5.0001 \theta) \cdot \theta^{\sum_{t=1}^{T} n_t}
\end{align*}

\noindent which implied that:
\[
\theta \mid \mathbf{n} \sim \text{Gamma}\left( \sum_{t=1}^{T} n_t + 1, 5.0001 \right)
\]

\subsection{Sampling Results}

Since all three posterior distributions were standard distributions, the Gibbs sampling method was employed to draw realizations from them\footnote{Markov Chain Monte Carlo (MCMC) methods, like Gibbs sampling, are used to draw samples from intractable posterior distributions. Gibbs sampling is efficient when full conditional posteriors are in closed form, often with conjugate priors. When posteriors are not in closed form, the Metropolis-Hastings algorithm can be used to generate candidate values from a proposal~distribution.}. This was implemented using the \texttt{JAGS} program, which was called from within~\texttt{R}. Following \citet{dudley2006bayesian}, three Markov chains were run in parallel. The initial values of \( \alpha \), \( \beta \), and \( \theta \) were~chosen to be well-dispersed and are presented in Table~\ref{tab:2}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Initial Parameter Values}
\label{tab:2}
\begin{threeparttable}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{Chain}}
*{3}{S[table-format=6.5]}
}
\hline
\textbf{Chain} & \( \alpha \) & \( \beta \) & \( \theta \) \\ \hline
1 & 0.00001 & 0.00001 & 0.00001 \\
2 & 100000 & 1 & 100000 \\
3 & 3.076 & 1.625 & 3.200 \\ \hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item These values are taken from \citet{dudley2006bayesian}.
\end{tablenotes}
\end{threeparttable}
\end{table}

A burn-in of 20,000 iterations was used. The statistics computed from the subsequent 30,000 iterations are presented in Table~\ref{tab:3}. A comparison with those reported by \citet{dudley2006bayesian} shows a close match, indicating that the model was properly specified and the Gibbs sampler executed correctly.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Posterior Statistics}
\label{tab:3}
\begin{threeparttable}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{\( E[Y] \)}}
*{2}{S[table-format=1.3]}
>{\raggedright\arraybackslash}p{\widthof{\textbf{95\% Bayesian Credible Interval}}}
}
\hline
 & \textbf{Mean} & \textbf{Standard Deviation} & \textbf{95\% Bayesian Credible Interval} \\ \hline
\( \alpha \) & 3.076 & 0.762 & (1.762, 4.752) \\
\( \beta \) & 1.591 & 0.035 & (1.498, 1.624) \\
\( \theta \) & 3.399 & 0.820 & (1.986, 5.185) \\
\( E[Y] \) & 2.507 & 1.071 & (2.024, 3.637) \\ \hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item \( E[Y] \) was calculated for each simulated set of parameters \( \alpha \) and \( \beta \), and from these values, the mean, standard deviation, and 95\% Bayesian credible interval were subsequently computed.
\end{tablenotes}
\end{threeparttable}
\end{table}

In addition, density plots were generated for each of the parameters and for \( E[Y] \), as presented in Figure~\ref{fig:1}. The resulting densities for the parameters resemble Gamma distributions, with the density of \( \beta \) appropriately truncated at \( \min\{y_{i,t}\} = 1.625 \). The plot for \( E[Y] \) displays a right-skewed distribution that permits very large values, albeit with very low probability---consistent with expectations.

\begin{figure}[!ht]
    \centering
    \caption{Posterior Densities}
    \label{fig:1}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/density_alpha.png}
        \subcaption{\( \alpha \)}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/density_beta.png}
        \subcaption{\( \beta \)}
    \end{minipage} \\
    
    \vspace{0.5cm}
    
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/density_theta.png}
        \subcaption{\( \theta \)}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/density_E_y.png}
        \subcaption{\( E[y] \)}
    \end{minipage}
    
\end{figure}

The posterior means of \( \alpha \) and \( \beta \) were used as parameters of the Pareto distribution, and the corresponding cumulative distribution function (CDF) was plotted against the empirical cumulative data (\( y_{i,t} \)). Similarly, the posterior mean of \( \theta \) was used as the parameter of the Poisson distribution, and its CDF was plotted against the empirical cumulative data (\( n_t \)). The Pareto\((3.079,\,1.592)\) distribution provides a close fit to the empirical data. The Poisson\((3.396)\) distribution also fits the observed frequencies quite well (see Figures \ref{fig:2} and \ref{fig:3}).

\begin{figure}[!ht]
    \centering
    \caption{Empirical vs.\ Fitted Pareto CDF}
    \label{fig:2}
    \includegraphics[width=.5\textwidth]{rytgaard1990/empirical_vs_pareto.png}
\end{figure}

\begin{figure}[!ht]
    \centering
    \caption{Empirical vs.\ Fitted Poisson CDF}
    \label{fig:3}
    \includegraphics[width=.5\textwidth]{rytgaard1990/empirical_vs_poisson.png}
\end{figure}

\subsection{Convergence Diagnostics}

Throughout all computations above, it was assumed that the Markov chains had converged to a stationary~distribution after discarding the initial 20,000 iterations, following the approach of \citet{dudley2006bayesian}. However, it was essential to verify that convergence had indeed occurred. The first method of assessment involved visual inspec- tion\footnote{Visual inspection involves assessing how well a chain explores the parameter space. Poor mixing—where the chain moves slowly or gets stuck—indicates potential convergence issues. Trace plots help identify such problems by showing how sampled values evolve over iterations for each parameter.}. Figure~\ref{fig:4} presents trace plots for all three parameters, showing the sampled values across iterations. These plots indicate good mixing, suggesting that the chains have likely converged.

\begin{figure}[!ht]
    \centering
    \caption{Trace Plots}
    \label{fig:4}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/trace_alpha.png}
        \caption{\( \alpha \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/trace_beta.png}
        \caption{\( \beta \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/trace_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}

\end{figure}

Convergence was further assessed using the Gelman--Rubin diagnostic \citep{Gelman1992}, applied to the post-burn-in iterations (20,001–50,000). The results, shown in Table~\ref{tab:4}, indicate that all univariate potential scale reduction factors (PSRFs) have point estimates and upper confidence bounds equal to 1. The multivariate PSRF is also 1. These values again suggest that the Markov chains have likely converged, both individually~and jointly.

\begin{table}[!ht]
\centering
\footnotesize
\caption{Potential Scale Reduction Factors (Gelman--Rubin Diagnostic)}
\label{tab:4}
\begin{threeparttable}
\begin{tabular}{
  >{\raggedright\arraybackslash}p{\widthof{\textbf{Multivariate PSRF}}}
  *{2}{S[table-format=1.2]}
}
\hline
\textbf{Parameter} & \textbf{Point Estimate} & \textbf{Upper C.I.} \\
\hline
\( \alpha \) & 1.00 & 1.00 \\
\( \beta \)  & 1.00 & 1.00 \\
\( \theta \) & 1.00 & 1.00 \\
\hline
\textbf{Multivariate PSRF} & \multicolumn{2}{c}{1.00} \\
\hline
\end{tabular}
\begin{tablenotes}
\footnotesize
\item The diagnostic was applied to iterations 20,001–50,000.
\end{tablenotes}
\end{threeparttable}
\end{table}

Figure~\ref{fig:5} shows how the univariate PSRF point estimates evolve with increasing iterations. Throughout all~iterations, all estimates remain below 1.1, which is commonly considered an acceptable threshold for convergence. This further confirms that the chains have likely reached a stable distribution.

\begin{figure}[!ht]
    \centering
    \caption{PSRF Values (Gelman--Rubin Diagnostic)}
    \label{fig:5}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/gelman_alpha.png}
        \caption{\( \alpha \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/gelman_beta.png}
        \caption{\( \beta \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/gelman_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}
\end{figure}

In addition, autocorrelation\footnote{The autocorrelation function (ACF) shows the correlation between samples at different lags. High autocorrelation indicates strong dependence between draws, leading to slow mixing. Thinning reduces this dependence by keeping only every \( k \)th sample.} plots were generated for all three parameters (see Figure~\ref{fig:6}), and values at~lags 1 through 10 are reported in Table~\ref{tab:5}. Several high autocorrelations were observed, particularly for \( \beta \), which~motivated the use of a thinning interval of 10 iterations, as suggested by \citet{dudley2006bayesian}.

\begin{figure}[!ht]
    \centering
    \caption{Autocorrelation Plots}
    \label{fig:6}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/acf_alpha.png}
        \caption{\( \alpha \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/acf_beta.png}
        \caption{\( \beta \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/acf_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}
\end{figure}

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Autocorrelations at Lags 1--10}
\label{tab:5}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{\textbf{Lag}}}
*{3}{S[table-format=-1.3]}
}
\hline
\textbf{Lag} & \( \alpha \) & \( \beta \) & \( \theta \) \\
\hline
1  & 0.288 & 0.699 & -0.001 \\
2  & 0.109 & 0.498 & -0.001 \\
3  & 0.053 & 0.356 & 0.003 \\
4  & 0.035 & 0.257 & -0.007 \\
5  & 0.019 & 0.191 & 0.002 \\
6  & 0.008 & 0.144 & -0.006 \\
7  & 0.010 & 0.104 & 0.003 \\
8  & 0.013 & 0.079 & -0.007 \\
9  & 0.007 & 0.061 & 0.001 \\
10 & 0.006 & 0.048 & 0.000 \\
\hline
\end{tabular}
\end{table}

Consequently, the chains were rerun with this thinning. Figure~\ref{fig:7} presents the corresponding trace plots, and Figure~\ref{fig:8} shows the updated autocorrelation plots. The trace plots indicate that the chains have mixed well,~and the autocorrelation plots demonstrate that all autocorrelation values at lags 1, 2, and beyond have~become~negligible.

\begin{figure}[!ht]
    \centering
    \caption{Trace Plots After Thinning}
    \label{fig:7}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/trace_after_thinning_alpha.png}
        \caption{\( \alpha \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/trace_after_thinning_beta.png}
        \caption{\( \beta \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/trace_after_thinning_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}
\end{figure}

\begin{figure}[!ht]
    \centering
    \caption{Autocorrelation Plots After Thinning}
    \label{fig:8}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/acf_after_thinning_alpha.png}
        \caption{\( \alpha \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/acf_after_thinning_beta.png}
        \caption{\( \beta \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rytgaard1990/acf_after_thinning_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}

\end{figure}

\subsection{Predictive Inference}

The ultimate objective of the analysis was to predict future values of \( S_t \), which was accomplished using the~posterior predictive distribution. For a variable \( z \), this distribution is defined as\footnote{This method accounts for the uncertainty in \( \boldsymbol{\phi} \) by integrating over its possible values, weighted by their posterior probabilities.}:
\[
\pi(z \mid \mathbf{y}) = \int_{\Theta} f(z \mid \mathbf{\phi}) \, \pi(\mathbf{\phi} \mid \mathbf{y}) \, d\mathbf{\phi}
\]

\noindent where \( \pi(\mathbf{\phi} \mid \mathbf{y}) \) is the posterior distribution of \( \mathbf{\phi} \), and \( f(z \mid \mathbf{\phi}) \) is the likelihood of \( z \) given \( \mathbf{\phi} \).

Let \( \mathbf{n} \) denote the observed data, and let \( N_f \) represent a future observation of \( N_t \). Then the posterior predic- tive distribution of \( N_f \) is given by:
\[
\begin{aligned}
p (N_f = n \mid \mathbf{n}) 
&= \int_0^{\infty} p(N_f = n \mid \theta) \pi(\theta \mid \mathbf{n}) \, d\theta \\
&= \mathbb{E}_{\theta \mid \mathbf{n}}\left[ p(N_f = n \mid \theta) \right] \\
&= \mathbb{E}_{\theta \mid \mathbf{n}}\left[ \frac{\theta^n e^{-\theta}}{n!} \right]
\end{aligned}
\]

\noindent Since the integral cannot be evaluated analytically, it is typically approximated using samples from the posterior distribution obtained via MCMC. Specifically, the expectation is approximated as:
\[
p (N_f = n \mid \mathbf{n}) \approx \frac{1}{m} \sum_{i=1}^{m} \frac{(\theta^{(i)})^n e^{-\theta^{(i)}}}{n!}
\]

\noindent where \( \theta^{(i)} \) is the \( i \)-th sample from the MCMC chain and \( m \) is the number of iterations after burn-in and thinning.

Similarly, let \( \mathbf{y} \) denote the observed data, and let \( Y_f \) represent a future observation of \( Y_{i,t} \). Then the posterior predictive cumulative distribution function (CDF) of \( Y_f \) is given by:
\[
\begin{aligned}
p (Y_f \leq y \mid \mathbf{y}) 
&= \int_{\mathbf{u}} p(Y_f \leq y \mid \mathbf{u}) \, \pi(\mathbf{u} \mid \mathbf{y}) \, d\mathbf{u} \\
&= \mathbb{E}_{\mathbf{u} \mid \mathbf{y}}\left[ p(Y_f \leq y \mid \alpha, \beta) \right]
\end{aligned}
\]

\noindent Again,
\[
p (Y_f \leq y \mid \mathbf{y}) \approx \frac{1}{m} \sum_{i=1}^{m} \left( 1 - \left( \frac{\beta^{(i)}}{y} \right)^{\alpha^{(i)}} \right)
\]

\noindent where \( \alpha^{(i)} \) and \( \beta^{(i)} \) denote the \( i \)-th samples from the MCMC chain.

Table~\ref{tab:6} shows the estimated probabilities of \( N_f = n \) for \( n = 0, \dots, 14 \), which are consistent with the findings of \citet{dudley2006bayesian}. Figure~\ref{fig:9} displays the estimated predictive probability density function (PDF) of \( Y_f \), obtained using the inverse CDF method\footnote{Specifically, 1,000 values \( U \sim \mathrm{Uniform}(0, 1) \) were generated. For each value, the transformation \(\beta^{(i)} (1 - U)^{-1/\alpha^{(i)}}\) was applied for each \( i \), and the results were averaged across \( i \). The resulting 1,000 values were then used to approximate the PDF of \( Y_f \) via~kernel density estimation (KDE).}.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Estimates of \( p (N_f = n \mid \mathbf{n}) \)}
\label{tab:6}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{\textbf{n}}}
S[table-format=1.4]
}
\hline
\( n \) & \textbf{Probability} \\ 
\hline
0  & 0.0453 \\ 
1  & 0.1282 \\ 
2  & 0.1919 \\ 
3  & 0.2023 \\ 
4  & 0.1683 \\ 
5  & 0.1177 \\ 
6  & 0.0718 \\ 
7  & 0.0393 \\ 
8  & 0.0196 \\ 
9  & 0.0091 \\ 
10 & 0.0039 \\ 
11 & 0.0016 \\ 
12 & 0.0006 \\ 
13 & 0.0002 \\ 
14 & 0.0001 \\ 
\hline
\end{tabular}
\end{table}

\begin{figure}[!ht]
\centering
\caption{Estimated Predictive PDF of \( Y_f \)}
\label{fig:9}
\includegraphics[width=0.5\textwidth]{rytgaard1990/predictive_Y_f.png}
\end{figure}

The draws of \( S_f \), representing a future observation of \( S_t \), were generated as follows:

\begin{enumerate}
\item 1,000 values of \( N_f \) were drawn using the inverse CDF method\footnote{Once again, 1,000 values \( U \sim \mathrm{Uniform}(0, 1) \) were generated. For each value, Poisson samples were drawn for each \( \theta^{(i)} \) using~the inverse CDF method. The results were averaged across \( i \) and rounded to produce 1,000 integer values.}.
\item For each simulated value of \( N_f \), the corresponding number of \( Y_f \) values was drawn using the same procedure as in the predictive PDF estimation, and these values were then summed to generate a draw of \( S_f \).
\end{enumerate}

\noindent Figure~\ref{fig:10} presents the histogram of the resulting \( S_f \) samples, along with three fitted distributions using moment matching. As observed by \citet{dudley2006bayesian}, the Gamma distribution provides the best fit. The fitted Gamma~dis- tribution has parameters \( \alpha = 1.726 \) and \( \beta = 0.202 \).

\begin{figure}[!ht]
\centering
\caption{Histogram and Fitted Distributions for Predictive \( S_f \)}
\label{fig:10}
\includegraphics[width=0.5\textwidth]{rytgaard1990/predictive_S_f.png}
\end{figure}

Table~\ref{tab:7} reports various percentiles of the simulated \( S_f \) values. The maximum value of 116.396 reflects~a long tail, consistent with expectations for a heavy-tailed claim size distribution. This indicates that the~simulation ef- fectively captured the tail behavior of the predictive distribution of \( S \), which is important since most aggregate claims are moderate, but extreme values can occasionally occur.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Percentiles of Simulated \( S_f \) Values}
\label{tab:7}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{99th Percentile}}
S[table-format=3.3]
}
\hline
\textbf{Percentile} & \textbf{Value} \\ 
\hline
Median & 7.504 \\ 
90th Percentile & 15.065 \\
95th Percentile & 18.092 \\
99th Percentile & 25.171 \\
Maximum & 116.396 \\
\hline
\end{tabular}
\end{table}

\section{Implementation on Alternative Data}

To assess the generalizability of the Bayesian approach proposed by \citet{dudley2006bayesian} for insurance claim modeling, the same methodology was applied to a different dataset. The \texttt{itamtplcost} dataset comprises 457 motor insur- ance claims exceeding 500{,}000 euros, recorded in Italy between 1997 and 2012 \citep{Dutang2020}.

Table~\ref{tab:8} presents a comparison of summary statistics between the \citet{rytgaard1990pareto} and \texttt{itamtplcost} datasets. Claims in the \texttt{itamtplcost} dataset are significantly larger, with a mean of 1.015~billion euros compared to~3.058 million euros in the \citet{rytgaard1990pareto} dataset. The variability is also greater, with a range of 6{,}637.34 versus~17.56. With regard to the number of claims per year, the \texttt{itamtplcost} dataset exhibits a minimum of 5 and a maximum of 40, while the \citet{rytgaard1990pareto} dataset ranges from 0 to 5. These differences highlight the greater complexity~of the \texttt{itamtplcost} dataset, providing a valuable opportunity to assess whether the Poisson--Pareto model~contin- ues to yield meaningful posterior inference in a more realistic insurance context.

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Comparison of Summary Statistics Between Datasets}
\label{tab:8}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{\citet{rytgaard1990pareto}}}
*{6}{S[table-format=4.3]}
}
\hline
\textbf{Dataset} & \textbf{Min} & \textbf{1st Qu.} & \textbf{Median} & \textbf{Mean} & \textbf{3rd Qu.} & \textbf{Max} \\
\hline
& \multicolumn{6}{c}{\textbf{Claim Amounts (in millions)}} \\
\hline
\citet{rytgaard1990pareto} & 1.625 & 1.745 & 1.863 & 3.058 & 2.109 & 19.180 \\
\texttt{itamtplcost} & 2.161 & 627.719 & 844.011 & 1015.352 & 1224.316 & 6639.500 \\
\hline
& \multicolumn{6}{c}{\textbf{Claim Counts}} \\
\hline
\citet{rytgaard1990pareto} & 0.00 & 3.00 & 4.00 & 3.20 & 4.00 & 5.00 \\
\texttt{itamtplcost} & 5.00 & 24.25 & 30.50 & 28.56 & 35.25 & 40.00 \\
\hline
\end{tabular}
\end{table}

Similarly to the previous case, Gibbs sampling was used to draw realizations from the posterior distributions of \(\alpha\), \(\beta\), and \(\theta\), with three Markov chains run in parallel and initial parameter values chosen to be well-dispersed. A burn-in of 5{,}000 iterations was initially applied, but the Gelman--Rubin diagnostic \citep{Gelman1992} suggested extending it to 10{,}000. Consequently, a burn-in of 10{,}000 was adopted, and autocorrelation plots~were generated. High autocorrelations were observed, prompting the use of a thinning interval of 10. The chains~were rerun with this thinning, and the trace plots, along with the updated autocorrelation plots, indicated good~mix- ing and negligible autocorrelation across all lags.

Despite this, all simulated values of \(\alpha\) were below 1, indicating that the mean of the Pareto distribution was infinite. This posed a risk of instability in the results and led to the conclusion that the Pareto--Poisson model was not suitable for the \texttt{itamtplcost} dataset. Consequently, alternative loss distributions were considered,~including the Weibull and lognormal. Among these, the lognormal distribution provided the best results.

The lognormal distribution was parametrized in terms of \(\mu\) and \(\tau\), where \(\tau = 1 / \sigma^2\). A vague normal prior~was assigned to \(\mu\), while vague gamma priors were used for \(\sigma\) and \(\theta\). Again, three Markov chains were run in parallel with well-dispersed initial values. For the third chain, the starting values were set to the maximum likelihood~es- timates (see Table~\ref{tab:9}).

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Initial Parameter Values (Poisson--Lognormal)}
\label{tab:9}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{Chain}}
*{3}{S[table-format=6.5]}
}
\hline
\textbf{Chain} & \( \mu \) & \( \sigma \) & \( \theta \) \\ \hline
1 & 0.00001 & 0.00001 & 0.00001 \\
2 & 100000 & 100000 & 100000 \\
3 & 6.700 & 0.802 & 28.563 \\ \hline
\end{tabular}
\end{table}

Initially, a burn-in of 1{,}000 iterations was employed, and the Gelman--Rubin diagnostic \citep{Gelman1992} was used to assess its adequacy. Figure~\ref{fig:11} illustrates the evolution of the univariate PSRF point estimates over the course of the iterations. Although all estimates remain below 1.1, they stabilize around 1 by approximately the 10{,}000\textsuperscript{th} iteration. As a result, a burn-in of 10{,}000 was adopted. The chains were then rerun, and autocorrelation plots were generated (see Figure~\ref{fig:12}), with values for lags 1 through 10 reported in Table~\ref{tab:10}. Moderate autocorrelations were observed, prompting the use of a thinning interval of 10.

\begin{figure}[!ht]
    \centering
    \caption{PSRF Values (Gelman--Rubin Diagnostic) (Poisson--Lognormal)}
    \label{fig:11}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/gelman_mu.png}
        \caption{\( \mu \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/gelman_tau.png}
        \caption{\( \tau \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/gelman_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}
\end{figure}

\begin{figure}[!ht]
    \centering
    \caption{Autocorrelation Plots (Poisson--Lognormal)}
    \label{fig:12}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/acf_mu.png}
        \caption{\( \mu \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/acf_tau.png}
        \caption{\( \tau \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/acf_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}
\end{figure}

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Autocorrelations at Lags 1--10 (Poisson--Lognormal)}
\label{tab:10}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{\textbf{Lag}}}
*{3}{S[table-format=-1.3]}
}
\hline
\textbf{Lag} & \( \mu \) & \( \tau \) & \( \theta \) \\
\hline
1  & 0.157 & 0.158 &  0.003 \\
2  & 0.035 & 0.040 &  0.003 \\
3  & 0.009 & 0.015 & -0.002 \\
4  & 0.003 & 0.006 & -0.003 \\
5  & 0.002 & 0.001 & -0.003 \\
6  & -0.002 & -0.001 & -0.002 \\
7  & -0.001 & -0.005 & -0.002 \\
8  & 0.002 & -0.006 & -0.001 \\
9  & 0.000 & 0.003 &  0.008 \\
10 & 0.004 & 0.006 & -0.003 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!ht]
    \centering
    \caption{Trace Plots After Thinning (Poisson--Lognormal)}
    \label{fig:13}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/trace_after_thinning_mu.png}
        \caption{\( \mu \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/trace_after_thinning_tau.png}
        \caption{\( \tau \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/trace_after_thinning_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}
\end{figure}

\begin{figure}[!ht]
    \centering
    \caption{Autocorrelation Plots After Thinning (Poisson--Lognormal)}
    \label{fig:14}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/acf_after_thinning_mu.png}
        \caption{\( \mu \)}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/acf_after_thinning_tau.png}
        \caption{\( \tau \)}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/acf_after_thinning_theta.png}
        \caption{\( \theta \)}
    \end{subfigure}

\end{figure}

Table~\ref{tab:11} reports statistics computed from 30{,}000 post-burn-in iterations, while Figure~\ref{fig:15} presents the posterior density plots for \( \mu \), \( \tau \), and \( \theta \).

\begin{table}[!ht]
\centering
\footnotesize
\setlength{\tabcolsep}{5pt}
\caption{Posterior Statistics (Poisson--Lognormal)}
\label{tab:11}
\begin{tabular}{
>{\raggedright\arraybackslash}p{\widthof{\( \theta \)}}
*{2}{S[table-format=2.3]}
>{\raggedright\arraybackslash}p{\widthof{\textbf{95\% Bayesian Credible Interval}}}
}
\hline
 & \textbf{Mean} & \textbf{Standard Deviation} & \textbf{95\% Bayesian Credible Interval} \\ \hline
\( \mu \) & 6.700 & 0.038 & (6.626, 6.774) \\
\( \tau \) & 1.552 & 0.103 & (1.358, 1.759) \\
\( \theta \) & 28.628 & 1.337 & (26.058, 31.313) \\ \hline
\end{tabular}
\end{table}

\begin{figure}[!ht]
    \centering
    \caption{Posterior Densities (Poisson--Lognormal)}
    \label{fig:15}
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/density_mu.png}
        \subcaption{\( \mu \)}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/density_tau.png}
        \subcaption{\( \tau \)}
    \end{minipage} \\
    
    \vspace{0.5cm}
    
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/density_theta.png}
        \subcaption{\( \theta \)}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{itamtplcost/density_E_y.png}
        \subcaption{\( E[y] \)}
    \end{minipage}
    
\end{figure}

The posterior means of \( \mu \) and \( \tau \) were used as the parameters of the lognormal distribution, and the corresponding cumulative distribution function (CDF) was plotted against the empirical cumulative data \( (y_{i,t}) \). Similarly, the posterior mean of \( \theta \) was used as the parameter of the Poisson distribution, and its CDF was plotted against the empirical cumulative data \( (n_t) \). The lognormal\((6.700,\,0.804)\) distribution provides a close fit to the empirical data, while the Poisson\((28.628)\) distribution also closely matches the observed frequencies (see Figures~\ref{fig:16} and~\ref{fig:17}).

\begin{figure}[!ht]
    \centering
    \caption{Empirical vs.\ Fitted Lognormal CDF (Poisson--Lognormal)}
    \label{fig:16}
    \includegraphics[width=.5\textwidth]{itamtplcost/empirical_vs_lognormal.png}
\end{figure}

\begin{figure}[!ht]
    \centering
    \caption{Empirical vs.\ Fitted Poisson CDF (Poisson--Lognormal)}
    \label{fig:17}
    \includegraphics[width=.5\textwidth]{itamtplcost/empirical_vs_poisson.png}
\end{figure}

Samples of \( Y_f \) were generated using the inverse CDF method as in the case of the \citet{rytgaard1990pareto} dataset. Figure~\ref{fig:18} displays the estimated predictive probability density function (PDF) of \( Y_f \).

\begin{figure}[!ht]
\centering
\caption{Estimated Predictive PDF of \( Y_f \) (Poisson--Lognormal)}
\label{fig:18}
\includegraphics[width=0.5\textwidth]{itamtplcost/predictive_Y_f.png}
\end{figure}

Then samples of \( S_f \) were simulated also in the same way as before. Figure~\ref{fig:10} presents the histogram of the resulting \( S_f \) samples, along with three fitted distributions using moment matching. The Gamma distribution seems to provide the best fit. The fitted Gamma distribution has parameters \( \alpha = 15.6920 \) and \( \beta = 0.0005 \).

\begin{figure}[!ht]
\centering
\caption{Histogram and Fitted Distributions for Predictive \( S_f \) (Poisson--Lognormal)}
\label{fig:19}
\includegraphics[width=0.5\textwidth]{itamtplcost/predictive_S_f.png}
\end{figure}


\printbibliography{references}
\end{document}